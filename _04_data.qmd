# Data

Our dataset was built by combining information from multiple sources, each selected for its relevance to MLB performance and wagering analysis. Together, these sources contributed complementary perspectives and enabled us to perform the kind of matchup-level analysis needed to explore profitability and uncover potential edges in the betting market.

## Data Sources
We collected data from five sources, which are outlined in greater detail in the following sections. StatsAPI, pybaseball, and oddsapi were accessed using a custom Python scraper that called their respective APIs directly, allowing us to ingest structured data on player performance, advanced metrics, and betting markets. This approach differed from our Selenium-based web scraper used later for MLB.com, which required automated browser interaction to extract dynamically loaded content not available through the APIs. Finally, we sourced supplemental tracking data from Baseball Savant by downloading static CSV files. Together, these pipelines enabled a comprehensive, multi-source dataset suitable for statistical analysis, machine learning, and real-world validation.

### statsapi
The first source was Major League Baseball’s public API, accessed using the statsapi Python package. This provided structured JSON data containing player metadata, team identifiers, game results, and detailed per-game statistics for batters and pitchers. This source served as the foundation for our dataset. To automate collection, we deployed the scraper on Railway, a cloud platform that lets us run and schedule scripts automatically without needing to manage any servers. We scheduled our scripts to runs daily at 7:00 AM PST, retrieving the previous day’s game data. The raw JSON was parsed using SQL queries in PostgreSQL and loaded into a set of normalized relational tables.

Our tables included players, teams, games, sides, batter_stats, and pitcher_stats. The players table contains a list of all individuals who appeared in at least one Major League Baseball game from June 21st to the present, each assigned a unique player_id for consistent identification. The teams table includes all 30 MLB teams, each mapped to a unique team_id, enabling team-level joins and analysis. The sides table identifies which team played as the home or away team in each game, an important distinction in baseball since the home team bats second and may have strategic advantages. The batter_stats and pitcher_stats tables store detailed per-game box score data, separated by hitting statistics and pitching statistics. These stats include traditional metrics such as hits, home runs, and strikeouts for batters, and innings pitched, earned runs, and walks for pitchers. The use of primary keys such as player_id, game_id, and team_id allowed for seamless joins across tables. This structure enables efficient querying across player appearances, game contexts, and performance metrics, while remaining flexible enough to incorporate additional data sources such as advanced Statcast metrics and other tracking metrics.


### mlb.com
Our second data source was MLB’s official website, mlb.com, which we scraped using Selenium, a Python package for automated browser interaction. This approach allowed us to extract pre-game matchup information from MLB.com’s preview pages, data not available through the public Stats API. Specifically, we captured scheduled starting pitchers and projected batter matchups, along with historical hitter performance against the opposing pitcher. It enabled us to factor in pre-game context and generate predictions ahead of each day’s games. The scraper was designed to click into each game’s “Preview” button, capture the relevant information, then return to the main page and repeat this process for all games scheduled on a given day. This scraper was deployed on Railway and scheduled to run slightly earlier than the StatsAPI job, at 6:55 AM PST. Unlike the StatsAPI scraper, it focused on collecting information for games scheduled later that same day.

A major challenge with this source was the absence of standardized identifiers such as player or game IDs. To address this, we created a composite key in the preview table using a combination of game_date, batter_name, and pitcher_name to uniquely identify each record. The JSON data gathered was unstructured so it required an extensive use of regular expressions (REGEX), which made the parsing process significantly more complex.

In the end, we produced a table called previews, which included the game date, projected starting pitcher, and historical performance data of each batter against that pitcher-- such as home runs, at-bats, and batting average. This table allowed us to enrich our dataset by linking valuable pre-game context with actual in-game performance, enabling more nuanced analyses of matchups and outcomes.

### Baseball Savant
To supplement our scraped data, we incorporated a third source: Baseball Savant, a publicly available website that provides detailed, season-long statistics that go beyond traditional box scores. Unlike our other sources, which offered game-by-game data, Baseball Savant's data is cumulative and tracks advanced metrics throughout the season. For example, it includes expected batting average which incorporates exit velocity (how hard the ball is hit) and launch angle (angle at which the player swings their bat) and pitch usage rates, which displays how frequently pitchers throw specific pitch types. The site offers the ability to download the data as CSV files, which we did during the All-Star break to capture a midseason snapshot of player performance. For the purposes of our analysis, we chose to download the data once, as the season is already more than halfway complete and player performance is unlikely to vary significantly over the remaining games. After downloading the CSV files, we cleaned the data by removing unnecessary columns and renaming fields to improve clarity and ensure consistency across our dataset. 

The processed data was stored in four normalized tables within our PostgreSQL database. The batter_pitches table captured how individual batters performed against specific pitch types—such as fastballs, sliders, and curveballs. It included advanced metrics like Run Value (a measure of how much a specific pitch type increases or decreases run expectancy when thrown to that specific batter) and expected batting average to estimate likely outcomes. Similarly, the pitcher_pitches table reflected how effective each pitcher was with their various pitch types, using the same set of metrics but from the pitcher’s perspective. The pitcher_statcast table provided aggregated performance statistics that were not pitch-specific, offering a broader view of each pitchers season-level tendencies. This Statcast data was particularly valuable as it combined pitch-level insights with overall performance metrics, enabling a more complete evaluation of both hitters and pitchers. 

### pybaseball
Our next source was the pybaseball Python package, which provided extended access to MLB’s Statcast system and allowed us to retrieve season-level statistics. This data enriched our dataset with advanced metrics, including batted ball tracking (which measures factors like exit velocity and hit direction), pitch characteristics, and sabermetric indicators (which provide expected values for more precise performance evaluation). Using pybaseball.statcast(), we collected pitch-level data spanning over a full season, which was then aggregated into player-game summaries. We were also able to extract seasonal statistics for batters, adding broader context around batter performance. 

To unify these sources, we used the players table to bridge the gap between the detailed Statcast tracking data and the rest of our dataset. By matching on player_id, as in earlier integrations, we successfully connected granular tracking metrics with season-level statistics for 537 unique players. This join allowed us to capture both game-level performance details and broader, long-term trends across the season.

Only historical data was needed, so our scraper ran once, and the collected data was stored in the batter_statcast table. This table followed a similar structure to pitcher_statcast, but included season-level summaries instead of data limited to the current year. All data was pushed into PostgreSQL, where this structure supported querying across pitch-level, game-level, and seasonal data, laying the groundwork for downstream modeling and analysis.

### oddsapi
Our betting market data was sourced using the OddsAPI feed, which provided structured JSON data on sportsbook offerings for player props such as hits, total bases, and home runs. This information was retrieved using the same custom scraper architecture used in other parts of the project. For consistency and integration with our game-level data, we joined odds records to player performance using a combination of game date and team abbreviations, ensuring alignment across sources despite the absence of shared unique identifiers.

The betting data was parsed and stored in PostgreSQL, using fields such as market_type (type of prop bet), price (decimal odds), and point (over/under threshold). Additional fields included bookmaker_key and last_update, allowing us to track line movement and source attribution. Although historical odds are not yet fully populated, this structure enables us to link sportsbook expectations directly to individual player-game performances. 

This integration supports downstream use cases such as model backtesting and value identification. This integration supports downstream use cases such as model backtesting and value identification. Backtesting refers to the process of applying a predictive model to historical data to evaluate how well it would have performed in real-world scenarios-- for example, simulating bets on past games based on model predictions and comparing the results to actual outcomes. Value identification involves comparing the model’s predicted probabilities for specific player outcomes (e.g., getting a hit or hitting a home run). By comparing predicted player outcomes to market-implied probabilities from sportsbooks, we can evaluate model performance not only in terms of accuracy but also potential profitability. This alignment creates a clear path for analysis centered on identifying inefficiencies in publicly posted betting lines.


## Data Organization
In the end the data was organized and combined using third normal form (3NF) to minimize redundancy and maintain reliable links across related data. Our entity-relationship diagram (ERD) (<a href="#fig-erd">Figure 1</a>) provides a clear visual representation, clearly mapping the relationships among all entities derived from our data acquisition process.

<figure id="fig-erd">
  [![ERD diagram](images/erd_diagram.png)](images/erd_diagram.png)
  <figcaption style="font-size: 10px; color: #555;">
    Figure 1: This diagram illustrates how our database schema is organized to support complex querying and analysis. At the center, the players, games, and teams tables form the relational backbone, linking to performance-specific tables like batter_stats, pitcher_stats, and their Statcast counterparts. The previews and player_props tables bring in pre-game context and betting market data, while sides defines home/away team roles per game. This structure, normalized to third normal form (3NF), allows us to seamlessly join historical performance, matchup context, and betting odds—enabling detailed statistical modeling, matchup evaluation, and value detection at both the player and game levels.
  </figcaption>
</figure>

[See Data Dictionary](#data-dictionary)