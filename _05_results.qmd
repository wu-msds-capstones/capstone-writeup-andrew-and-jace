# Results

## Over/Under Run Scoring Analysis
Our initial analysis focused on identifying which variables were most predictive of how many runs a team scores in a given game. We used a wide range of features grouped into three main categories:

- **Opposing starting pitcher (sp.)**: Including metrics such as earned run average (ERA), opponent batting average (BA), innings pitched (IP), and expected stats like xERA and xBA.

- **Opposing relief pitcher statistics (rp.)**: Represented aggregated bullpen performance using the same types of measures as for starters, such as ERA and slugging percentage (SLG) allowed.

-- **Team batter statistics (b.)**: Covered both per-game aggregates (e.g., total hits, home runs, strikeouts) and season-long averages for the lineup, such as batting average (BA) and slugging percentage (SLG).

By integrating data from opposing starting pitchers, opposing relievers, and the team’s offensive performance, we aimed to construct a well-rounded model for predicting team run performance.

### Exploratory Data Analysis

To gain insight into team scoring behavior, we computed summary statistics for runs scored per game. This helped us examine the distribution of offensive output and understand typical scoring ranges across all team-game observations.

<figure id="fig-run_dist">
  <img src="images/run_dist.png" alt="Run Distribution" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 2**: The distribution of runs scored per game across all teams from June 21 to the present is right-skewed, with most teams scoring between 3 and 7 runs. Extreme high-scoring games (10+ runs) are relatively rare, while shutouts and 1-run games are pretty common.
  </figcaption>
</figure>

The distribution of runs scored per game, as seen in <a href="#fig-run_dist">Figure 2</a>, was right-skewed, with a few high-scoring outliers pulling the mean upward. The median was 4 runs, while the mean was slightly higher at 4.6, reflecting the impact of those outliers. This mean value serves as a useful benchmark when modeling run production in machine learning, especially when deciding how to frame our prediction target.

We then focused on identifying which features were most indicative of how many runs a team scores in a game. To do this, we built a linear regression model using a wide range of pitching and hitting statistics. However, many baseball statistics are derived from overlapping components, making them inherently correlated. For example, batting average is one of several inputs used to calculate weighted on-base average (wOBA), meaning these two variables are mathematically linked. As a result, including both in a predictive model can introduce multicollinearity, where highly correlated inputs distort the model’s ability to accurately estimate the unique contribution of each feature. To address this, we systematically filtered out redundant variables, prioritizing those that captured unique predictive value. After this reduction process, we finalized a simplified linear model using the following variables:
starting pitcher earned run average (`sp_era`), starting pitcher batting average against (`sp_ba`), starting pitcher slugging percentage against (`sp_slg`), relief pitcher earned run average (`rp_era`), relief pitcher slugging percentage against (`rp_slg`), relief pitcher innings pitched (`rp_ip`), team batting average (`b_ba`), total team strikeouts (`b_sum_so`), and team slugging percentage (`b_slg`).

| Predictor | Estimate | Std. Error | t value | Pr(>|t|) |
|--------------|--------------|----------------|-------------|--------------|
| (Intercept) | 6.538 | 2.377 | 2.751 | 0.0061 ** |
| sp_era | -0.135 | 0.130 | -1.042 | 0.2980 |
| sp_ba | 4.744 | 6.150 | 0.771 | 0.4408 |
| sp_slg | -4.021 | 3.252 | -1.236 | 0.2168 |
| rp_era | -0.220 | 0.095 | -2.323 | 0.0205 * |
| rp_slg | 3.594 | 2.329 | 1.543 | 0.1232 |
| rp_ip | 0.224 | 0.089 | 2.527 | 0.0117 * |
| b_ba | 26.823 | 11.862 | 2.261 | 0.0241 * |
| b_sum_so | 0.050 | 0.045 | 1.130 | 0.2589 |
| b_slg | 9.894 | 5.692 | 1.738 | 0.0826 . |

<figure id="tab-var">
  <figcaption style="font-size: 10px; color: #555;">
    **Table 1**: Coefficient estimates from a linear regression model predicting team runs scored. The model includes variables related to starting and relief pitcher performance (e.g., ERA, innings pitched, opponent batting metrics), as well as team-level offensive statistics such as batting average, slugging percentage, and total strikeouts. Each coefficient reflects the estimated change in runs scored associated with a one-unit increase in the corresponding predictor, holding all other variables constant. Statistically significant predictors (p < 0.05) are marked with an asterisk (*), indicating a meaningful relationship with run production.
  </figcaption>
</figure>

From our linear model, we identified three statistically significant predictors of runs scored (p < 0.05):

- Relief pitcher innings pitched (`rp_ip`), with a p-value of 0.0117

- Relief pitcher ERA (`rp_era`), with a p-value of 0.0205

- Team batting average (`b_ba`), with a p-value of 0.0241

Batting average stood out as something worth exploring further, especially as the season progresses, since we also have data on expected batting average (xBA) available.  

### Actual vs. Expected Batting Average
The law of averages suggest that after a large number of trials, outcomes tend to stabilize and move closer to their long-run tendencies. In the context of baseball, this implies that a team’s observed batting average may begin to reflect their underlying hitting ability over time. While not a mathematical guarantee, this principle provides context for using modeled statistics. Model-derived metrics are not a true statistical expectation, but it does serve as a useful benchmark for identifying under- or over-performance across the season.

Expected batting average is a Statcast metric designed to estimate the likelihood that a batted ball will result in a hit, which refers to any batted ball that results in the batter safely reaching at least first base without the benefit of an error. The metric is based on two key factors: exit velocity (how hard the ball is hit) and the launch angle (the vertical angle, in degrees, at which the ball leaves the bat relative to the ground). Baseball results can be unpredictable and are influenced by baseball alignments. As a result, traditional stats do not always measure how well a player is actually performing. That is where expected statistics come in-- through data they provide a more accurate picture of a player’s performance. For example, a hitter might crush a ball at 115 MPH, but if it’s hit directly at a defender, it results in an out. On the other hand, a softly hit ball at just 40 MPH could drop into the perfect spot for a hit. xBA accounts for these inconsistencies by using historical data to estimate how often similar batted balls have gone for hits. While not perfect, it offers a more context-aware view of hitting performance. The following graph, (<a href="#fig-ba">Figure 3</a>), compares each team’s actual batting average to their xBA, helping us see who has been overperforming or underperforming based on quality of contact.

<figure id="fig-ba">
  <img src="images/ba.png" alt="Batting Average" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 3**: Comparison of actual batting average (BA) and expected batting average (xBA) for all MLB teams. Each point represents a team’s seasonal averages. The dashed diagonal line indicates where a team's actual BA equals its xBA-- meaning they are performing as expected based on the quality of their contact. Teams plotted above the line are overperforming their expected average (higher BA than xBA would suggest), while those below the line are underperforming (recording a lower BA than expected).
  </figcaption>
</figure>

Since we are trying to understand team run production it makes sense to focus on batting average. A higher team batting average means the team is collecting more hits on average, which is essential for generating offense. In baseball, scoring runs typically requires advancing base runners, and hits are one of the primary ways to move players around the bases. The more often players get on base via hits, the more opportunities a team has to bring runners home. While not every hit results in a run, consistent hitting increases the chances of building rallies and ultimately scoring.  When a team’s actual batting average is lower than its expected batting average (xBA), it may suggest underperformance, assuming xBA reasonably reflects the quality of contact. While xBA is not a perfect estimate, it offers a standardized way to evaluate whether a team is hitting into bad luck or failing to convert quality contact into results. Over time, if xBA is a reliable reflection of a team's underlying hitting quality, we would expect actual outcomes to begin aligning with those expectations. Teams underperforming their xBA may see an uptick in batting average, and consequently, run production. Conversely, teams overperforming may experience a decline in their runs as fewer balls in play fewer batted balls result in hits over time. To explore whether these trends actually play out over time, we examined how average run production shifted after MLB's All-Star Break (ASB) as shown in (<a href="#fig-run_diff">Figure 4</a>). 

<figure id="fig-run_diff">
  <img src="images/run_diff.png" alt="Run Differences" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 4**: Change in average runs scored per game for MLB teams after the 2024 All-Star Break (ASB) compared to before the break. Positive values (green bars) indicate teams that increased their run production, with the Chicago White Sox (+5.15) showing the largest improvement. Negative values (red bars) indicate teams that scored fewer runs on average after the break, with the Boston Red Sox (-4.33) experiencing the steepest decline. The figure highlights the five teams with the largest increases and decreases, illustrating which offenses surged or struggled to start the second half of the season.
  </figcaption>
</figure>

A pattern emerges when comparing team-level batting average (BA) and expected batting average (xBA) to changes in run production after the All-Star Break. Among the largest movers, several teams that had been underperforming their xBA—such as CLE, CWS, KC, and HOU—saw significant increases in runs scored (<a href="#fig-run_diff">Figure 4</a>), consistent with the idea of positive regression. Conversely, some overperforming teams like BOS and SEA experienced declines in run production, aligning with expectations of regression toward the mean. However, the relationship was far from universal-- teams like DET and BAL deviated from the expected pattern. This indicates that while xBA may highlight potential for regression, its predictive power appears strongest for certain teams at the extremes and less reliable across the league as a whole. (<a href="#fig-ba2">Figure 5</a>).

The Colorado Rockies (COL) appear to be overperforming their expected batting average (xBA), but this is likely a product of Coors Field-- MLB’s most hitter-friendly park. The high altitude and thin air in Denver inflate offensive stats by reducing air resistance and allowing balls to travel farther. This "Coors Field Effect" regularly skews batting metrics, making Colorado’s apparent overperformance more a reflection of park conditions than unsustainable hitting. This highlights the importance of considering contextual factors-- such as ballpark effects, strength of schedule, and injuries when interpreting performance metrics. In future work, it would be valuable to explore other factors to add to the depth.  

<figure id = "fig-ba2">
  <img src="images/ba2.png" alt="Batting Average Overlayed" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 5**:  Relationship between changes in average runs scored after the All-Star Break and each team’s degree of overperformance or underperformance relative to their expected batting average (xBA). Green indicates teams whose run production has increased, while red indicates teams whose run production has declined.
  </figcaption>
</figure>

## Machine Learning for Over/Unders
Following the xBA and run production analysis, we were curious whether setting a clear scoring threshold could reveal additional predictive signals. We framed over/under outcomes in MLB games as a binary classification task-- predicting whether a team would score more or fewer than 4.5 runs. This threshold closely mirrors the historical league average of 4.6 runs, providing a relevant and interpretable baseline for evaluation.

We selected a Random Forest algorithm for our classification model due to its strong performance and interpretability. Random Forests are ensemble models that reduce overfitting by aggregating the results of multiple decision trees, leading to more accurate predictions. Additionally, they offer clear insights into feature importance, allowing us to understand which variables most influence model outcomes.

### Feature Engineering
We engineered two key features: the average number of runs a team has scored over its last five games (`rolling_runs_5`) and the team’s batting average over the same span `team_rolling_ba_5`. These metrics serve as short-term performance indicators, capturing both a team’s ability to generate runs and its overall hitting effectiveness in recent matchups. By focusing on a five-game window, they provide a timely snapshot of offensive form that can reflect hot streaks, slumps, or the impact of recent roster changes.

Additionally, we incorporated several existing metrics into our model, including the opponent starting pitcher’s expected batting average (sp_x_ba), opponent starting pitcher’s earned run average (sp_era), the starting lineup’s average expected batting average (b_x_ba), and team slugging percentage (b_slg).

### Model Performance 
To evaluate the effectiveness of our Random Forest classifier in predicting whether a team would score over or under 4.5 runs, we examined several performance metrics that offer complementary insights into model quality and reliability.

The Area Under the Receiver Operating Characteristic Curve (AUC) was 0.756, reflecting strong discriminative performance. This indicates that, when comparing a randomly selected game in which a team scored over 4.5 runs to one in which it did not, the model assigns a higher probability to the over outcome approximately 76% of the time. Our targeted result can be influenced by a wide range of dynamic and interrelated variables, thus an AUC above 0.75 indicates the model is reliably distinguishing between high and low scoring outcomes. This result suggests that the model is effectively capturing underlying patterns that differentiate high-scoring from low-scoring team performances. This performance is further illustrated by the ROC curve in <a href="#fig-roc">Figure 6</a>, which visualizes the model’s trade-off between sensitivity and specificity across all classification thresholds. The steep initial rise and bowed shape of the curve indicate effective separation between the two classes—teams that scored over 4.5 runs and those that did not—culminating in an AUC of 0.756. This reinforces the model’s reliability in identifying scoring patterns relevant to over/under predictions.

<figure id = "fig-roc">
  <img src="images/roc.png" alt="ROC Graph" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 6**: ROC curve for the Random Forest model predicting whether a team scores over 4.5 runs. The curve illustrates the trade-off between sensitivity (true positive rate) and specificity (true negative rate). The model shows strong performance, with the curve rising well above the diagonal line 
  </figcaption>
</figure>

In addition to AUC, overall accuracy provides a more intuitive sense of model correctness. The model achieved an overall accuracy of 72.5%, correctly predicting the outcome in nearly three-quarters of all games. This far exceeds the No Information Rate of 55.8%, which reflects the accuracy one would achieve by always predicting the majority class (in this case teams scoring under 4.5 runs). The substantial lift over this baseline demonstrates that the model is capturing meaningful patterns in the data rather than simply echoing the most frequent outcome. This improvement is particularly noteworthy given the variability of baseball scoring, where small changes in game context, player performance, or even weather conditions can swing results.

We also evaluated sensitivity, which measures how well the model identifies games where a team scored over 4.5 runs. It achieved a rate of 76.6%, meaning that when a team did go over, the model predicted it correctly more than 75% of the time. This level of performance is especially useful in betting contexts, where one would need to predict the over correctly more than 52.4% of the time just to break even. The model’s specificity was 67.2%, indicating that it also performed well at identifying games that went under. 

Finally, we considered Cohen’s Kappa, which was 0.44. Unlike raw accuracy, Kappa adjusts for the possibility of correct predictions occurring by chance, offering a more rigorous measure of model agreement. A value of 0.44 indicates moderate agreement beyond chance, reinforcing that the model captures real predictive signals. While not exceptionally high, this level of Kappa still demonstrates that the model performs meaningfully better than random guessing—an important outcome given the inherent variability when predicting team run totals.

Together, these results show that our Random Forest model performs well across both interpretability and predictive accuracy dimensions, making it a valuable tool for forecasting team run production in MLB games.

### Feature Importance
<figure id = "fig-var">
  <img src="images/var.png" alt="Important Variables for O/U ML" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 7**: This figure illustrates the relative importance of each feature in predicting whether a team will score over 4.5 runs. Higher values indicate greater contribution to the model’s predictive accuracy.
  </figcaption>
</figure>

The variable importance plot highlights the relative contribution of each feature to the model’s predictive accuracy. Among all inputs, the engineered feature <span style="color: #1f77b4;">rolling_runs_5</span> overwhelmingly emerged as the most influential. This feature, which captures a team's recent scoring momentum, proved to be the strongest indicator of whether a team would surpass the 4.5 run threshold. Its high importance reinforces the value of short-term performance trends in forecasting offensive output.

Following this, <span style="color: #1f77b4;">team_rolling_ba_5</span>, which represents the team's batting average over the last five games, also ranked highly. This measure captures the overall quality of a team’s hitting during recent matchups, serving as a gauge of a team's ability to consistently get hits.

Traditional pitching and matchup-based metrics also contributed meaningfully. For instance, <span style="color: #1f77b4;">sp_x_ba</span> and <span style="color: #1f77b4;">sp_era</span>, which reflect the expected batting average and earned run average of the opposing starting pitcher, respectively, helped capture the quality of pitching faced by the offense. These features provide critical context for how difficult it might be for a team to score in a given game.

Similarly, <span style="color: #1f77b4;">b_x_ba</span> (expected batting average of the lineup) and <span style="color: #1f77b4;">b_slg</span> (slugging percentage) contributed additional insight into the underlying hitting power and contact quality of the offensive side.

Overall, the combination of short-term offensive momentum, recent team batting metrics, and opposing pitcher quality formed the backbone of the model’s predictive framework. These findings suggest that blending engineered features with traditional matchup statistics enhances the model’s ability to predict over/under outcomes accurately.



## Prop Betting Analysis for Batters
We then explored hitter-focused prop bets, specifically whether a batter would record at least one hit or go over/under 1.5 total bases in a given game. Total bases are calculated as one for a single, two for a double, three for a triple, and four for a home run. To hit the over, a player must total at least two bases-- for example, by hitting a double, triple, home run, or combining multiple hits such as a single and a double.

### Pitcher-Batter Preview 
To gain an edge in predicting favorable prop outcomes, we began by analyzing game preview data for each day, including scheduled starting pitchers and historical batter-vs-pitcher matchup statistics. 

The first areas we explored was how specific batters had performed against certain pitchers in the past. In baseball, it is often said that some hitters "have a pitcher's number," meaning they see a pitcher very well— a phenomenon that may not always be captured by traditional stats. This can stem from a batter's ability to pick up the ball exceptionally well out of a pitcher’s hand or from a strong sense of that pitcher’s tendencies. Thus, when a batter is able to consistently succeed against a particular pitcher, it can signal a meaningful matchup advantage. With that in mind, we prioritized these historical trends as a way to identify potentially favorable prop bet opportunities.

We analyzed discrepancies in run value per 100 pitches (RV/100)-- a metric measuring how each pitch changes a team’s run expectancy, with positive values favoring hitters and negative values favoring pitcher. This would aid us in identifying potential pitcher-hitter mismatches. This allowed us to identify potential pitcher-hitter mismatches as we calculated the difference in RV/100 by comparing a batter’s performance against specific pitch types to the different pitches thrown by the scheduled opposing pitcher. This allowed us to estimate how well a batter might match up based on the types of pitches they were likely to face in a given game. In other words, we cross-referenced each batter’s strengths with a pitcher’s weaknesses to uncover matchups where a hitter may be especially well-suited to succeed. To ensure the matchup was meaningful, we also applied a minimum usage threshold, filtering out pitch types that a pitcher rarely throws.

#### Interactive Hitter Matchup App
We wanted a way to translate our findings into a practical, game-day resource, we developed an interactive dashboard designed to point out the most advantageous hitter macthups for each day's slate of games. The tool identifies situations where a hitter has a statistical edge and allows users to examine supporting statistics, such as matchup histories or run-value differentials. 

The top table displays advantageous batter–pitcher matchup histories, filtered to highlight hitters with the strongest track records. We included only hitters with a batting average of .300 or higher against the opposing pitcher-- a mark generally considered very good in baseball, indicating the hitter records a hit in at least 30% of their past at-bats. To ensure meaningful results, we also required a minimum of four at-bats in the matchup to provide a more reliable sample size.

The table below highlights batters with a significant advantage based on run value against specific pitch types. It is sorted from highest to lowest run value difference, showcasing hitters who are most likely to succeed against the opposing pitcher's arsenal. To qualify for this table, a batter must have a run value differential of at least 2.5 in their favor. If a batter appears in both this table and the matchup history table above, they are highlighted in yellow-- indicating that both analyses suggest a strong likelihood of success in that day’s game.

Users can sort the dashboard by date, with the default view automatically displaying matchupss with supporting stats from selected day’s slate of games. If a previous date is selected, historical matchups and their corresponding results will appear. A color-coded key is provided below the date selector to help interpret the meaning of each highlighted row.

At the bottom of the dashboard is a simulated betting analysis. For players to get a hit,it is typically priced around -200 odds, meaning a $200 wager would win $100 in profit. For the purpose of our analysis, we will be assuming a $10 wager per player. A successful bet yields a $5 profit, while a loss forfeits the $10 stake. For total bases (2+), where odds are usually closer to +100, we again simulate $10 wagers. In this case, a win returns $10 in profit, while a loss results in a $10 loss. The dashboard aggregates daily results based on these assumptions and displays the cumulative outcomes for easy tracking.

<div style="width: 100vw; margin-left: calc(-50vw + 50%); text-align: center;">
  <iframe src="https://60bwvg-jace-higa.shinyapps.io/best_hitter_matchups/" 
          width="100%" height="1000px" style="border:none;"></iframe>
  <p style="margin-top: 8px; font-style: italic; font-size: 0.95em; color: #555;">
    <strong>Figure 8:</strong> Interactive dashboard showing advantageous MLB batter props, with hypothetical results displayed as well.
  </p>
</div>

## Machine Learning for Individual Player Props

While the interactive dashboard approach provided valuable insights into daily matchup advantages, we recognized the need for a scalable, data-driven approach to individual player prop prediction. To complement the human-guided analysis, we developed a comprehensive machine learning pipeline focused on predicting specific prop betting outcomes using historical player performance and advanced statistical modeling.

### Statistical Thinking and Model Development

Our machine learning approach was grounded in established sports betting research, particularly the findings of Walsh & Joshi (2024) demonstrating that model calibration matters more than raw accuracy for betting profitability. Rather than optimizing solely for traditional machine learning metrics, we prioritized probability calibration to ensure our predictions would translate effectively to real-world betting scenarios.

We framed individual player prop prediction as a series of binary classification problems, creating target variables that directly corresponded to common betting markets: whether a player would record more than 0.5 hits (`hits_over_0.5`), more than 1.5 hits (`hits_over_1.5`), and more than 1.5 total bases (`total_bases_over_1.5`). This approach aligned our statistical models with practical betting applications while maintaining methodological rigor.

A central emphasis throughout our model development was avoiding data leakage to ensure our predictions would replicate information realistically available before game time. This methodological focus guided our feature engineering process and model validation approach, prioritizing realistic predictive capability over artificially inflated performance metrics.

### Feature Engineering and Data Leakage Prevention

Our feature engineering process was designed with strict adherence to temporal constraints that would apply in real-world betting scenarios. We exclusively used information that would be available before game time, avoiding any same-game statistics that could artificially inflate model performance.

From seasonal batting statistics, we incorporated established player ability metrics including `season_batting_avg`, `season_obp`, `season_slg`, `season_ops`, `season_woba`, and `season_wrc_plus`. These provide baseline context for each player's demonstrated skill level over the current season.

To capture per-game expectations without using same-game data, we calculated historical rate features such as `historical_hr_per_game`, `historical_hit_per_game`, `historical_walk_rate`, and `historical_k_rate` by dividing season totals by games played. These features provide realistic estimates of typical player output while maintaining temporal validity.

Experience and volume indicators included `games_played`, `season_games`, and contextual factors such as `is_home` and `season` indicators. These features capture player experience, health, and situational factors without incorporating any information from the game being predicted.

The final dataset comprised 19,587 player-game records across training (12,000), validation (4,000), and test (3,587) sets, with 47% of records containing complete seasonal statistical context. This represented a deliberate trade-off between data completeness and prediction reliability, focusing on players with sufficient historical information for meaningful modeling.

### Model Architecture and Algorithm Selection

Based on academic literature review, we implemented a comparative analysis of multiple machine learning algorithms known to perform well in sports prediction contexts. Our model suite included Random Forest, XGBoost, and ensemble methods, each calibrated using isotonic regression to optimize probability estimates for betting applications.

The Random Forest implementation used 100 estimators with a maximum depth of 6, configured conservatively to prevent overfitting in the challenging sports prediction environment. Our XGBoost configuration employed 100 estimators with a learning rate of 0.05 and regularization parameters designed specifically for sports betting applications.

Most critically, following Walsh & Joshi's research demonstrating that calibration-optimized models achieved +34.69% ROI versus -35.17% for accuracy-focused models, we applied CalibratedClassifierCV with isotonic regression to all base models. This ensured our probability outputs would be well-calibrated for betting decision-making rather than optimized solely for classification accuracy.

To leverage the strengths of multiple algorithms, we created ensemble models using weighted voting that combined the calibrated outputs of our individual models. This approach aligned with Galekwa et al. (2024) findings that ensemble methods consistently outperform single models in sports betting applications.

### Model Performance and Validation

Our comprehensive evaluation employed temporal validation with strict separation between training, validation, and test periods to simulate realistic prediction scenarios. Training data covered April-May 2025, validation used June 2025, and testing employed July 2025 data, ensuring models never saw future information during training.

The Random Forest model achieved the strongest performance with an average test AUC of 0.549 across target variables, followed by ensemble methods at 0.548 AUC and XGBoost at 0.548 AUC. Individual target performance varied, with `hits_over_1.5` achieving the highest AUC of 0.554, while `total_bases_over_1.5` recorded 0.543 AUC.

<figure id="fig-model_performance">
  <img src="images/model_performance_comparison_final.png" alt="Model Performance Comparison" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 9**: Model performance comparison across MLB player prop targets. Left panel shows all algorithms clustering near the academic baseline (Elfrink 2018), demonstrating realistic performance expectations for sports betting applications. Right panel validates absence of overfitting through strong validation-test agreement along the diagonal.
  </figcaption>
</figure>

When benchmarked against established academic literature, our results align closely with the documented baseline of 0.552 AUC (Elfrink, 2018) for MLB prediction tasks. This performance falls within the expected range for legitimate sports betting models that avoid data leakage, demonstrating that our emphasis on methodological rigor produced realistic performance estimates rather than artificially inflated metrics.

The consistency of performance across different target variables and temporal periods validates our feature engineering approach and suggests that our models captured generalizable patterns in player performance rather than overfitting to specific outcomes or time periods. As shown in <a href="#fig-model_performance">Figure 9</a>, the tight clustering of all algorithms around the academic baseline, combined with the strong validation-test agreement along the diagonal, confirms both realistic performance expectations and robust generalization capabilities.

### Feature Importance and Model Interpretation

Analysis of feature importance revealed that seasonal performance metrics provided the primary predictive signal, with `season_batting_avg`, `season_ops`, and `season_woba` consistently ranking among the top contributors across all models. This finding validates the importance of established player ability as captured through comprehensive seasonal statistics.

<figure id="fig-feature_importance">
  <img src="images/feature_importance.png" alt="Feature Importance Analysis" width="800">
  <figcaption style="font-size: 10px; color: #555;">
    **Figure 10**: Feature importance analysis aggregated across all models and targets, demonstrating the dominance of legitimate pre-game predictors. Seasonal statistics (blue) and historical rates (orange) provide the strongest predictive signal, while the complete absence of same-game features confirms successful data leakage prevention.
  </figcaption>
</figure>

Historical rate calculations such as `historical_hr_per_game` and `historical_hit_per_game` proved valuable for normalizing seasonal performance across different playing time contexts. Experience indicators including `games_played` provided meaningful context about player reliability and health status.

As demonstrated in <a href="#fig-feature_importance">Figure 10</a>, the absence of any same-game derived features from the importance rankings confirms our successful elimination of data leakage while maintaining predictive capability. The dominance of seasonal statistics and historical rates validates our interpretation that established player ability, rather than game-specific circumstances, drives the primary predictive signal in our models.

Contextual factors like `season` and `is_home` showed moderate importance, suggesting systematic differences in performance across these dimensions. The absence of any same-game derived features from the top importance rankings confirms our successful elimination of data leakage while maintaining predictive capability.

### Model Calibration and Odds Data Integration

A critical aspect of our modeling approach was ensuring well-calibrated probability outputs suitable for betting applications. Our calibration analysis demonstrated that all models produced probability estimates closely aligned with observed outcome frequencies, essential for comparing model predictions with betting market prices.

The ROC analysis confirmed consistent discriminative ability across different target variables, with all models achieving meaningful separation between positive and negative cases despite the challenging nature of individual player prediction.

### Planned Integration with Betting Market Data

To evaluate whether our statistical models can identify profitable betting opportunities, we will integrate our calibrated probability predictions with real-time odds data from multiple sportsbooks. This integration represents the critical next phase of our hybrid modeling approach, where statistical predictions meet market-based information.

Our odds data infrastructure captures player prop lines from major sportsbooks including DraftKings, FanDuel, BetMGM, and Caesars across the key markets we predict: hits over 0.5, hits over 1.5, and total bases over 1.5. By implementing comprehensive odds shopping across these platforms, we can identify the most favorable lines available for each predicted outcome, maximizing potential value when our model probabilities diverge significantly from market prices.

The integration methodology will compare our calibrated model probabilities directly with implied probabilities from betting lines, accounting for sportsbook margins to identify genuine value opportunities. For instance, if our model assigns a 0.70 probability to a player recording 1+ hits, but the best available odds imply only a 0.60 probability (after margin adjustment), this represents a potential value bet. Consistent application of this approach across hundreds of daily prop opportunities will test whether our statistical edge translates to profitable betting outcomes.

### Integration with Matchup Analysis

Our machine learning approach complements the interactive matchup analysis by providing automated screening capabilities across all players and games. While the dashboard excels at identifying specific daily opportunities through pitcher-batter historical data and pitch-type analysis, the ML models can evaluate hundreds of prop opportunities simultaneously using consistent statistical criteria.

This integration allows ML models to identify players whose predicted performance substantially differs from betting market prices, while the dashboard provides contextual validation through matchup-specific insights. The hybrid approach leverages both algorithmic pattern recognition capabilities and nuanced situational analysis for comprehensive opportunity identification.
